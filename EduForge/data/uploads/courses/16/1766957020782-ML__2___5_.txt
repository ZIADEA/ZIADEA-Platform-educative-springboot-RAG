COURS COMPLET : MACHINE LEARNING – Concepts, Méthodes et Applications
Module 1 : Fondamentaux et Vision d'Ensemble
1.1 Qu'est-ce que le Machine Learning ?
Le Machine Learning (apprentissage automatique) est une discipline de l'intelligence artificielle qui permet aux systèmes informatiques d'apprendre à partir des données, sans être explicitement programmés pour chaque tâche. Au lieu de coder des règles fixes, on fournit des exemples à un algorithme qui découvre les patterns et les régularités sous-jacentes.
Analogie fondamentale : Apprendre à distinguer une pomme d'une orange. Plutôt que de définir des règles complexes (« si ronde et rouge, alors pomme »), on montre des milliers d'exemples étiquetés. L'algorithme capture la variabilité naturelle : les pommes vertes, les oranges rouges, les formes irrégulières.
1.2 Les Trois Paradigmes Principaux
A. Apprentissage Supervisé
Le type le plus fréquent. Les données fournies contiennent les étiquettes (labels) désirées. L'algorithme apprend à prédire ces étiquettes pour de nouveaux exemples.
Deux sous-catégories :
Classification : Prédire une catégorie discrète (malade/sain, spam/non-spam, chiffre manuscrit de 0 à 9)
Régression : Prédire une valeur continue (prix d'un bien immobilier, température demain, durée d'un trajet)
B. Apprentissage Non Supervisé
Aucune étiquette fournie. L'algorithme découvre la structure cachée dans les données.
Tâches principales :
Clustering : Regrouper des observations similaires (segmentation clients, détection d'anomalies)
Réduction de dimension : Simplifier les données tout en conservant l'information essentielle (visualisation, compression)
Extraction de règles : Identifier des associations (clients qui achètent X achètent souvent Y)
C. Apprentissage par Renforcement
Déjà traité dans un cours dédié, mais à situer comme le troisième pilier. L'agent apprend par interaction avec un environnement, guidé par des récompenses.
1.3 Le Cycle de Vie d'un Projet Machine Learning
Phase 1 : Compréhension du problème
Traduire le besoin métier en question ML claire. Exemple : « Réduire les fraudes » devient « Prédire si une transaction est frauduleuse à partir de ses caractéristiques ».
Phase 2 : Collecte et préparation des données
Identifier les sources (bases de données, APIs, capteurs)
Nettoyer les valeurs manquantes, aberrantes
Transformer les données au bon format
Cette phase représente souvent 70-80% du temps réel
Phase 3 : Exploration et compréhension
Analyses statistiques descriptives
Visualisations des distributions, des corrélations
Identification des patterns visuels
Cette phase guide les choix algorithmiques
Phase 4 : Modélisation
Sélection d'algorithmes candidats
Entraînement sur les données d'apprentissage
Réglage des paramètres (hyperparamètres)
Validation croisée pour évaluer la robustesse
Phase 5 : Évaluation et validation
Tester sur des données jamais vues (ensemble de test)
Vérifier les métriques de performance
Validation métier : Le modèle répond-il au besoin initial ?
Phase 6 : Déploiement et monitoring
Intégration dans un système de production
Surveillance de la performance dans le temps
Détection du dérive conceptuelle (quand les données réelles changent)
Maintenance et réentraînement périodique
Module 2 : Les Données – Nerf de la Guerre
2.1 Types et Structures de Données
Données Tabulaires
Format tableur/ligne-colonne
Chaque ligne : une observation (ex : un client)
Chaque colonne : une caractéristique/feature (ex : âge, revenu)
Le format le plus répandu en entreprise
Données Textuelles
Documents, emails, tweets, avis
Nécessite une vectorisation (transformation en nombres)
Exemples d'approches : sac de mots, embeddings, transformers
Données Images
Pixels organisés en matrices 2D ou 3D (couleurs)
Haute dimensionnalité (millions de pixels)
Nécessite des réseaux de neurones convolutifs
Données Séquentielles
Séries temporelles (cours boursiers, température)
Signaux (ECG, audio)
Langage (séquence de mots)
Ordonnées temporellement, pas indépendantes
Données Graphiques
Réseaux sociaux, molécules chimiques, infrastructures
Relations entre entités (nœuds et arêtes)
2.2 Le Data Preprocessing – Art et Nécessité
Gestion des valeurs manquantes
Plusieurs stratégies possibles :
Supprimer les lignes avec trop de valeurs manquantes
Imputer par la moyenne, médiane ou valeur la plus fréquente
Prédire les valeurs manquantes avec un modèle simple
Risque : Introduire du biais si mal fait
Gestion des valeurs aberrantes
Détection par méthodes statistiques (écart à la moyenne)
Visualisation graphique (boîtes à moustaches)
Décision : Conserver (signal) ou corriger (bruit) selon le contexte
Encodage des variables catégorielles
One-hot encoding : Créer une colonne binaire par catégorie
Ordinal encoding : Assigner des numéros (attention aux faux ordres)
Target encoding : Utiliser la statistique de la variable cible
Normalisation et standardisation
Mettre toutes les caractéristiques à la même échelle
Importance : Beaucoup d'algorithmes sont sensibles aux échelles
Standardisation : Centrer-réduire (moyenne 0, écart-type 1)
Normalisation : Ramener entre 0 et 1
Sélection des caractéristiques
Identifier les variables informatives vs. redondantes
Corrélation avec la variable cible
Importance mutuelle entre features
Risque : La colinéarité (variables très corrélées entre elles)
2.3 Le Jeu de Données – Séparation Stratégique
Pourquoi séparer ?
Évaluer la capacité de généralisation du modèle : sa performance sur de nouvelles données.
Ensemble d'apprentissage (Train)
60-80% des données
Utilisé pour apprendre les patterns
Ensemble de validation (Validation)
10-20% des données
Utilisé pour régler les hyperparamètres
Joue le rôle de fausses données de test pendant le développement
Ensemble de test (Test)
10-20% des données
N'utiliser qu'une seule fois à la fin
Donne l'estimation réelle de la performance en production
Stratégies de séparation
Séparation aléatoire simple (si données indépendantes)
Séparation temporelle (pour séries chronologiques)
Séparation par groupe (pour données groupées, ex : patients)
Module 3 : Algorithmes et Familles
3.1 Les Arbres de Décision – Intuition et Simplicité
Principe de fonctionnement
Un arbre pose des questions binaires successives pour aboutir à une décision.
Exemple : « Âge > 30 ? » → Oui → « Revenu > 50000 ? » → Oui → « Prêt accordé »
Avantages
Très intuitif et interprétable (on peut visualiser l'arbre)
Gère naturellement variables numériques et catégorielles
Peu de prétraitement nécessaire
Rapide à entraîner
Inconvénients
Tendance au surapprentissage (apprend le bruit)
Instable : Petit changement dans les données → arbre très différent
Crée des frontières de décision rectangulaires (pas toujours optimal)
Extensions puissantes
Forêts aléatoires : Combinaison de centaines d'arbres sur des sous-ensembles aléatoires
Gradient Boosting (XGBoost, LightGBM) : Arbres construits séquentiellement pour corriger les erreurs des précédents
Ces extensions sont souvent les algorithmes les plus performants sur données tabulaires
3.2 Les Machines à Vecteurs de Support (SVM)
Concept central
Trouver l'hyperplan qui sépare le mieux les classes avec la plus grande marge.
« Hyperplan » = généralisation d'une ligne droite en 2D ou d'un plan en 3D.
Version linéaire vs non-linéaire
Linéaire : Quand les classes sont séparables par une ligne droite
Non-linéaire : Utilise le « kernel trick » pour transformer l'espace de données
Kernels populaires
Linéaire : Pour données déjà linéairement séparables
Polynômial : Pour des frontières courbes simples
RBF (Radial Basis Function) : Très flexible, capture des frontières complexes
À retenir
Très puissant en petite dimension
Sensibles au choix du kernel et à la calibration
Moins utilisé sur très grandes bases de données
3.3 La Régression Logistique – Malgré son Nom
Attention au piège
Malgré « régression », c'est un algorithme de classification binaire.
Intuition
Modélise la probabilité d'appartenance à une classe grâce à la fonction sigmoïde (forme en S). On obtient une probabilité entre 0 et 1.
Utilisations typiques
Spam detection
Diagnostic médical (malade/sain)
Prédiction de churn (client qui va partir)
Avantages
Rapide, simple, interprétable
Donne des probabilités (utile pour décision métier)
Bien calibré avec peu de données
3.4 Les K Plus Proches Voisins (K-NN)
Principe ultra-simple
Pour classer un nouveau point, regarde les K points les plus proches dans l'espace. La classe majoritaire parmi ces voisins gagne.
K = paramètre critique
K petit : Frontières complexes, risque de surapprentissage
K grand : Lissage excessif, risque de sous-apprentissage
Distance
Comment mesure la proximité ? Distance euclidienne classique, mais d'autres existent.
Inconvénients majeurs
Très lent en prédiction (doit calculer toutes les distances)
Nécessite toutes les données en mémoire
Sensibles à l'échelle des features
3.5 Les Méthodes Bayésiennes Naïves
Fondement théorique
Théorème de Bayes : Met à jour la probabilité d'une hypothèse avec de nouvelles preuves.
Hypothèse « naïve »
Suppose que les caractéristiques sont indépendantes les unes des autres (rarement vrai, mais fonctionne souvent !).
Types populaires
Multinomial : Pour données de comptage (fréquence de mots)
Bernoulli : Pour données binaires
Gaussien : Pour données continues
Cas d'usage typique
Filtrage de spam (classique historique)
Classification de documents
Analyse de sentiment
Qualité
Très rapide
Peu de données nécessaires
Donne des probabilités interprétables
3.6 Les Réseaux de Neurones Artificiels – Vers le Deep Learning
Perceptron de base
Unité élémentaire qui prend des entrées, applique des poids, et active si le seuil est dépassé.
Multi-layer Perceptron (MLP)
Combinaison de plusieurs couches de perceptrons :
Couche d'entrée
Couches cachées (1 à plusieurs)
Couche de sortie
Apprentissage par rétropropagation
Ajuste progressivement les poids pour minimiser l'erreur de prédiction.
Pourquoi « deep » ?
Quand on a beaucoup de couches cachées (dizaines, centaines), le réseau peut apprendre des représentations hiérarchiques complexes.
Domaines d'excellence
Vision par ordinateur (images)
Traitement du langage naturel (texte)
Reconnaissance vocale
Jeux stratégiques complexes
Module 4 : Concepts Fondamentaux de Performance
4.1 Surapprentissage (Overfitting) – Le Fléau
Définition
Le modèle apprend par cœur les données d'apprentissage, y compris leur bruit, et échoue à généraliser.
Symptômes
Forte performance sur train, faible sur test/validation
Variabilité importante selon les jeux de données
Causes
Modèle trop complexe pour la quantité de données
Entraînement trop long
Bruit dans les données
Stratégies de prévention
Régularisation : Pénaliser la complexité du modèle
Dropout : Désactiver aléatoirement des neurones pendant l'entraînement
Augmentation de données : Créer artificiellement de nouveaux exemples
Validation précoce : Arrêter l'entraînement quand la validation dégrade
Simplifier le modèle : Moins de paramètres, moins de features
4.2 Sous-apprentissage (Underfitting)
Définition
Le modèle est trop simple pour capturer les patterns importants dans les données.
Symptômes
Faible performance sur train et test (similaires)
Le modèle ne capture pas les relations évidentes
Solutions
Complexifier le modèle
Ajouter des features plus informatives
Réduire la régularisation
Entraîner plus longtemps
4.3 Dérive Conceptuelle (Concept Drift)
Problème réel
Les relations dans les données évoluent dans le temps. Un modèle qui marchait hier peut dégrader demain.
Exemples concrets
Évolution des fraudes bancaires (nouvelles techniques)
Changement des préférences consommateurs
Évolution des maladies (nouveaux variants)
Solutions
Monitoring continu de la performance
Réentraînement périodique
Pondération des exemples récents plus fortement
Algorithmes adaptatifs (online learning)
4.4 Validation Croisée – Estimation Robuste de Performance
Pourquoi ?
Évaluer la performance sur une seule séparation train-test peut être trompeur (trop de variance).
Principe
Diviser les données en K blocs (folds). Itérer K fois : chaque fois, un bloc différent sert de validation, les autres d'entraînement. Moyenner les performances.
Avantages
Utilise toutes les données pour l'entraînement et la validation
Estimation plus stable de la performance
Détecte l'instabilité du modèle
Types
Validation croisée K-fold (standard)
Stratified K-fold (préserve les proportions des classes)
Leave-one-out (extrême, pour petits jeux de données)
Module 5 : Métriques d'Évaluation
5.1 Pour la Régression
Erreur Quadratique Moyenne (MSE)
Moyenne des carrés des erreurs. Pénalise fortement les grosses erreurs.
Erreur Absolue Moyenne (MAE)
Moyenne des valeurs absolues des erreurs. Plus robuste aux outliers.
R² (Coefficient de Détermination)
Proportion de la variance expliquée par le modèle. Entre 0 et 1 (ou parfois négatif). Plus proche de 1 = meilleur.
Attention
Pour la régression, toujours regarder les résidus (erreurs) visuellement pour détecter des patterns.
5.2 Pour la Classification Binaire
Matrice de Confusion – Outil Central
Tableau 2x2 :
Vrais Positifs (VP) : Prédit positif, réellement positif
Faux Positifs (FP) : Prédit positif, réellement négatif
Vrais Négatifs (VN) : Prédit négatif, réellement négatif
Faux Négatifs (FN) : Prédit négatif, réellement positif
Exactitude (Accuracy)
Pourcentage de prédictions correctes. (VP+VN) / Total
Limitation majeure : Sur des données déséquilibrées (ex : 99% de négatifs), un modèle qui prédit toujours négatif a 99% d'exactitude mais est inutile.
Précision (Precision)
Proportion de vrais positifs parmi les positifs prédits. VP / (VP+FP)
Indique la fiabilité quand le modèle prédit positif.
Rappel (Recall) ou Sensibilité
Proportion de vrais positifs détectés. VP / (VP+FN)
Indique la capacité à trouver tous les cas positifs.
F1-Score
Moyenne harmonique de précision et rappel. Équilibre les deux métriques.
Utile quand les classes sont déséquilibrées.
Courbe ROC et AUC
ROC : Graphique du rappel vs taux de faux positifs pour différents seuils
AUC : Aire sous la courbe ROC. Métrique globale entre 0 et 1 (0.5 = aléatoire)
Spécificité
Proportion de vrais négatifs correctement identifiés. VN / (VN+FP)
5.3 Pour la Classification Multi-classes
Exactitude globale
Même formule, mais sur plus de classes.
Moyenne par classe
Macro-average : Moyenne des métriques par classe (donne même poids à chaque classe)
Micro-average : Calcule les métriques globalement (privilégie les classes majoritaires)
Rapport de classification
Tableau affichant précision, rappel, F1 pour chaque classe individuellement.
5.4 Pour le Clustering
Coefficient de silhouette
Mesure à la fois la cohésion intra-cluster et la séparation inter-cluster.
Entre -1 et 1. Plus proche de 1 = meilleur.
Indice de Rand ajusté
Compare le clustering obtenu avec la vérité terrain (si disponible).
Corrigé pour le hasard.
Inertie intra-cluster
Somme des distances au centroïde. À minimiser.
Module 6 : Biais et Éthique
6.1 Les Types de Biais
Biais de sélection
Les données ne représentent pas la population réelle. Ex : modèle de reconnaissance faciale entraîné uniquement sur visages caucasiens.
Biais de mesure
Erreurs systématiques dans la collecte. Ex : capteur mal calibré qui sature à certaines valeurs.
Biais de confirmation
Le modèle apprend à reproduire les préjugés présents dans les données historiques. Ex : recrutement qui favorise historiquement les hommes.
Biais d'omission
Certaines populations sous-représentées. Ex : données médicales manquantes pour certains groupes ethniques.
6.2 Conséquences Pratiques
Discrimination algorithmique
Systèmes de recrutement qui pénalisent certaines origines
Credit scoring qui défavorise des quartiers
Justice prédictive avec biais raciaux
Boucles de rétroaction
Un système de recommandation qui renforce les préférences, créant une bulle de filtre.
Transparence et explicabilité
Modèles « boîte noire » (réseaux profonds) vs interprétables (arbres)
Droit à l'explication (RGPD en Europe)
6.3 Stratégies d'Atténuation
Audit pré-déploiement
Tester le modèle sur différents sous-groupes
Mesurer les disparités de performance
Équité (Fairness)
Définir des métriques d'équité (parité, égalité des chances)
Ajuster le modèle ou les décisions pour les respecter
Données représentatives
Assurer la diversité dans l'échantillonnage
Sur-échantillonner les sous-groupes sous-représentés
Humain dans la boucle
Ne pas automatiser des décisions sensibles sans supervision
Maintien d'un contrôle humain final
Module 7 : Bonnes Pratiques et Workflow
7.1 Gestion des Projet ML
Versionnage des données
Utiliser DVC (Data Version Control) ou équivalent
Suivre quelles données ont servi à entraîner quel modèle
Reproductibilité absolue
Versionnage des expériences
MLflow, Weights & Biases, Neptune
Suivre hyperparamètres, métriques, artefacts
Comparer facilement les runs
Environnements reproductibles
Docker pour conteneuriser les environnements
requirements.txt ou environment.yml pour les dépendances
7.2 Feature Engineering – L'Art du ML
Création de features
Transformer des variables existantes (log, racine carrée)
Extraire des informations (jour de la semaine d'une date)
Combiner des variables (ratio entre deux features)
Importance des features
Identifier les variables les plus informatives
Feature selection : Éliminer les redondantes
Risque : Leakage (utiliser une information qui ne serait pas disponible au moment de la prédiction)
7.3 Réglage des Hyperparamètres
Grid Search
Tester toutes les combinaisons possibles dans une grille prédéfinie.
Avantage : Exhaustif. Inconvénient : Coûteux.
Random Search
Tester des combinaisons aléatoires.
Souvent plus efficace que grid search pour trouver de bonnes configurations.
Bayesian Optimization
Utilise les résultats précédents pour choisir intelligemment le prochain test.
Plus efficace, mais plus complexe.
7.4 Tests et Validation
Tests unitaires
Vérifier que le preprocessing ne casse pas
Tester les shapes des données en entrée/sortie
Vérifier les distributions des prédictions
Tests d'intégration
Tester la pipeline complète (data → modèle → prédiction)
Vérifier les dépendances
Validation A/B
En production, tester la nouvelle version contre l'ancienne sur un sous-ensemble d'utilisateurs.
Module 8 : Limites et Défis
8.1 La Nécessité des Données
« Garbage in, garbage out »
Un mauvais dataset produit inévitablement un mauvais modèle, quel que soit l'algorithme.
Quantité vs Qualité
Besoin de suffisamment d'exemples pour chaque pattern
Mais la qualité est primordiale : données bruitées, biaisées, ou mal étiquetées dégradent tout
8.2 Le Coût Caché
Coût de collecte et d'annotation
Annoter des données peut être extrêmement coûteux (ex : segmentation médicale)
Nécessite souvent des experts domaine
Coût écologique
Entraîner de grands modèles (GPT, Bert) consomme énormément d'énergie
Choix d'algorithmes plus sobres quand possible
Coût de maintenance
Un modèle en production nécessite un monitoring continu
Reentraînement régulier
Risque d'obsolescence technique
8.3 Les Limites Fondamentales
Impossibilité d'extrapolation
Un modèle ne peut prédire que ce qui ressemble à ce qu'il a vu. Les événements « black swan » restent imprévisibles.
Corrélation vs Causalité
Le ML capture des corrélations, pas des relations causales. Une corrélation n'implique pas causalité.
Le paradoxe de l'interprétabilité
Les modèles les plus performants (réseaux profonds) sont les moins interprétables. Les modèles simples sont moins puissants mais plus transparents.
8.4 Le Mythe de la « Solution Unique »
Pas d'algorithme universellement meilleur
Le « No Free Lunch Theorem » : Sur l'ensemble de tous les problèmes possibles, tous les algorithmes sont équivalents en moyenne.
Nécessité d'expérimentation
Il faut toujours tester plusieurs approches, car le meilleur algorithme dépend des caractéristiques spécifiques du dataset.
Module 9 : Écosystème et Outils
9.1 Bibliothèques Principales
Scikit-learn – La Référence Python
Implémente la majorité des algorithmes classiques
API unifiée et cohérente (fit, predict, transform)
Excellente documentation et exemples
Limité pour le deep learning
TensorFlow / Keras
Framework Google pour deep learning
Keras : Interface haut niveau, très accessible
Production-ready (TensorFlow Serving)
PyTorch
Framework Facebook/Meta, très populaire en recherche
API plus « pythonique » et intuitive
Excellente pour expérimenter rapidement
Pandas – Manipulation de Données
Tableaux de données (DataFrames)
Opérations de filtrage, groupement, fusion
Incontournable pour le preprocessing
NumPy – Calcul Numérique
Tableaux multidimensionnels
Opérations mathématiques optimisées
Fondation de quasiment toutes les librairies ML
9.2 Outils de Visualization
Matplotlib
Bibliothèque de base, très flexible
Courbes, histogrammes, scatter plots
Seaborn
Surcouche à Matplotlib
Graphiques statistiques plus beaux par défaut
Plotly
Graphiques interactifs
Utiles pour les dashboards
TensorBoard
Visualisation d'entraînement des réseaux de neurones
Courbes de loss, métriques, histogrammes de poids
9.3 Plateformes Cloud
AWS SageMaker
Inférence et entraînement managed
Notebooks Jupyter intégrés
Google AI Platform / Vertex AI
Intégration avec l'écosystème Google
AutoML pour automatiser la sélection de modèles
Microsoft Azure ML
Interface visuelle (drag-and-drop)
Bonne intégration avec outils Microsoft
Kaggle Kernels / Google Colab
Environnements gratuits (avec GPU limité)
Parfaits pour apprendre et expérimenter
Module 10 : Conseils Pédagogiques et Projets Étudiants
10.1 Stratégie d'Enseignement Progressif
Semaines 1-2 : Découverte
Manipuler des datasets simples (Iris, Titanic)
Utiliser Scikit-learn avec API de haut niveau
Focus sur l'intuition et l'interprétation
Semaines 3-5 : Approfondissement
Comprendre les métriques en détail
Feature engineering manuel
Comparer plusieurs algorithmes sur un même problème
Semaines 6-8 : Complexification
Données déséquilibrées
Gestion du surapprentissage
Validation croisée et réglage d'hyperparamètres
Semaines 9-10 : Deep Learning Introduction
Perceptron simple
Réseau de neurones shallow sur données tabulaires
Semaines 11-12 : Projet Libre
Groupe de 3-4 étudiants
Choix d'un dataset réel (Kaggle)
Toute la pipeline complète
10.2 Projets Types par Niveau
Débutant (solo)
Prédire la survie sur le Titanic
Classer les fleurs d'Iris
Détecter des emails spam
Intermédiaire (petit groupe)
Prédire les prix immobiliers
Classer des articles de news par catégorie
Segmenter des clients pour une entreprise
Avancé (groupe, plusieurs semaines)
Reconnaissance de chiffres manuscrits (MNIST) avec réseau convolutif
Analyse de sentiment sur tweets
Système de recommandation simple (films, produits)
10.3 Critères d'Évaluation d'un Projet ML
Qualité du code
Propreté, documentation, modularité
Usage de fonctions et classes
Rigueur de la démarche
Séparation train-test respectée
Validation croisée
Pas de leakage
Analyse critique
Discussion des limites du modèle
Identification des biais potentiels
Propositions d'amélioration
Communication
Notebook clair et narratif
Visualisations pertinentes
Conclusions métier explicites
10.4 Ressources Complémentaires
Livres
"The Hundred-Page Machine Learning Book" – Andriy Burkov (concis)
"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow" – Aurélien Géron (pratique)
"Pattern Recognition and Machine Learning" – Christopher Bishop (théorique)
Cours en ligne
"Machine Learning" de Andrew Ng (Coursera – classique)
"CS229" de Stanford (plus avancé)
Chaîne YouTube "StatQuest" pour visualisations
Communautés
Kaggle (datasets, compétitions, forums)
Stack Overflow (questions techniques)
Reddit r/MachineLearning (actualités)
Sites de datasets
UCI Machine Learning Repository (classique)
Kaggle Datasets (vaste et varié)
Google Dataset Search (moteur de recherche)
Module 11 : Perspectives et Évolution du Domaine
11.1 Automatisation du ML (AutoML)
Objectif : Automatiser la sélection d'algorithmes, le preprocessing, le réglage d'hyperparamètres.
Outils
Auto-sklearn, TPOT, H2O AutoML
Peut-être vu comme un « ML sur le ML »
Limites
Coûteux en calcul
Moins flexible qu'un expert humain
Risque de solutions « black box »
11.2 Apprentissage Continu (Continual Learning)
Les modèles apprennent de nouvelles tâches sans oublier les anciennes.
Défi : L'oubli catastrophique (catastrophic forgetting).
11.3 Apprentissage Fédéré (Federated Learning)
Entraîner des modèles sur des données restant localement (ex : téléphones) sans centraliser les données.
Avantages : Confidentialité, réduction des coûts de transfert.
11.4 Transformer le ML
Les modèles Transformers, initialement pour le NLP, se généralisent :
Vision (Vision Transformer)
Audio (Whisper)
Multi-modalités (texte + image)
11.5 Science des Données vs ML Enginering
Data Scientist : Concentre sur l'exploration, la modélisation, l'analyse.
ML Engineer : Concentre sur la production, le déploiement, l'échelle, la robustesse.
Evolution : Les frontières se brouillent, les compétences de production deviennent essentielles même pour les data scientists.
Conclusion Systémique
Le Machine Learning n'est pas une boîte magique mais une discipline rigoureuse qui requiert :
Une compréhension profonde des données
Une pensée critique constante sur les biais et les limites
Une itération expérimentale systématique
Une communication claire des résultats et incertitudes
L'expertise ne vient pas de la maîtrise d'un seul algorithme, mais de la capacité à choisir la bonne approche pour chaque problème spécifique, à anticiper les écueils, et à traduire les résultats techniques en impact métier tangible.
Le plus important : Le ML est un outil au service de la résolution de problèmes réels. La question « Pourquoi » (le problème métier) prime toujours sur le « Comment » (l'algorithme).