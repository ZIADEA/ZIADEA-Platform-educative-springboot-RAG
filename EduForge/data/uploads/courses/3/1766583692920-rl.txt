Module 1 : Fondements et Concepts Clés
1.1 Définition et Philosophie
L'apprentissage par renforcement est une branche de l'intelligence artificielle où un agent apprend à prendre des décisions en interagissant avec un environnement. Contrairement à l'apprentissage supervisé où on fournit des exemples étiquetés, ici l'apprentissage se fait par essai et erreur, guidé par des récompenses et des punitions.
Analogie pédagogique : Imaginez un enfant qui apprend à marcher. Il fait un pas (action), observe s'il avance ou tombe (état suivant), et reçoit un feedback (joie d'avancer vs. douleur de tomber). Progressivement, il optimise sa "politique" de marche.
1.2 Les Cinq Composants Essentiels
A. L'Agent (Agent)
L'entité qui apprend et prend des décisions
Peut être un robot, un logiciel, un joueur de jeux
Possède une "politique" qui guide ses choix
B. L'Environnement (Environment)
Le monde avec lequel l'agent interagit
Peut être simulé (jeu vidéo) ou réel (route pour une voiture autonome)
Répond aux actions de l'agent
C. Les États (States)
Représentation de la situation actuelle
Exemple : position d'un joueur sur un échiquier, image perçue par une caméra
L'espace des états peut être fini ou infini
D. Les Actions (Actions)
Choix possibles pour l'agent à chaque état
Exemple : avancer, reculer, tourner à gauche
Peut être discret (quelques boutons) ou continu (angle de volant)
E. Les Récompenses (Rewards)
Signal scalaire immédiat reçu après chaque action
Crucial : C'est le seul signal d'apprentissage !
Une récompense positive renforce un comportement, une négative le punit
1.3 Processus de Décision Markovien (MDP)
Le MDP est le cadre mathématique qui formalise le problème. Même sans formules, comprenez qu'il garantit que l'état suivant ne dépend que de l'état actuel et de l'action prise (propriété de Markov).
Intuition : Pour décider quoi faire maintenant, vous n'avez pas besoin de tout l'historique, juste de la situation présente.
 Module 2 : Les Trois Approches Principales
2.1 Méthodes Basées sur la Valeur (Value-Based)
Principe : Apprendre à évaluer chaque situation plutôt que d'agir directement.
Idée centrale : On calcule la "valeur" de chaque état ou état-action : "Si je suis ici et que je fais ça, à long terme, combien de récompense puis-je espérer ?"
Q-Learning (Apprentissage Q) :
Crée une "table Q" qui stocke la qualité de chaque action dans chaque état
Démarre avec des valeurs aléatoires, puis les affine par exploration
Phase d'exploration : Essayer de nouvelles actions (faire des erreurs)
Phase d'exploitation : Utiliser ce qui marche le mieux
Dilemme exploration/exploitation : Le coeur du challenge !
Analogie : Vous essayez différents restaurants dans une ville. Au début, vous explorez aléatoirement. Progressivement, vous apprenez quels restaurants sont bons (valeur élevée) et y revenez plus souvent.
2.2 Méthodes Basées sur la Politique (Policy-Based)
Principe : Apprendre directement la politique (la stratégie) sans évaluer les états intermédiairement.
Policy Gradient :
Paramétrise la politique directement (ex : probabilité de chaque action)
Ajuste les paramètres pour maximiser les récompenses attendues
Plus stable sur les grands espaces d'actions continus
Avantage : Plus naturel pour les actions continues (ex : tourner le volant de 17.3°).
2.3 Méthodes Acteur-Critique (Actor-Critic)
Principe : Combiner les deux approches précédentes.
Deux rôles :
L'Acteur : Décide quelle action prendre (policy-based)
Le Critique : Évalue si la décision était bonne (value-based)
Processus :
L'acteur propose une action
Le critique évalue l'action ("C'était mieux que prévu !")
L'acteur ajuste sa stratégie selon le feedback du critique
Analogie : Un élève (acteur) répond à une question, le professeur (critique) donne un feedback constructif, l'élèves' améliore.
 Module 3 : Apprentissage par Renforcement Profond (Deep RL)
3.1 Quand les Réseaux de Neurones Entrent en Jeu
Le Deep RL combine le RL avec des réseaux de neurones pour résoudre des problèmes avec des espaces d'états immenses (images, sons).
Problème de la "malédiction de la dimensionnalité" :
Un jeu d'échecs a plus d'états qu'atomes dans l'univers observable
Impossible de stocker une table Q dans la mémoire
Solution : Utiliser un réseau neuronal qui approxime la fonction de valeur ou la politique.
3.2 Algorithmes Clés du Deep RL
DQN (Deep Q-Network) :
Utilise un CNN pour estimer les valeurs Q à partir des pixels du jeu
Experience Replay : Stocke les expériences passées pour les réutiliser (échantillonnage efficace)
Target Network : Utilise deux réseaux pour stabiliser l'apprentissage
Révolution : A battu des champions humains au jeu Atari juste en regardant les pixels
A3C (Asynchronous Advantage Actor-Critic) :
Utilise plusieurs agents en parallèle qui apprennent simultanément
Chaque agent a une copie de l'environnement et échange périodiquement ses connaissances
Avantage : Plus rapide, plus stable, utilise mieux les CPU
PPO (Proximal Policy Optimization) :
Algorithme de référence aujourd'hui pour de nombreuses tâches
Très stable, facile à régler (hyperparamètres robustes)
Utilisé par OpenAI pour des robots simulés et des tâches de contrôle
Démonstration visuelle conceptuelle : Imaginez un robot qui apprend à marcher. Au lieu de programmer chaque muscle, on le laisse essayer des milliers de fois. Son "cerveau" neuronal reçoit les récompenses (distance parcourue) et ajuste les connexions pour améliorer la démarche.
 Module 4 : Applications Concrètes et Captivantes
4.1 Jeux et Détente
AlphaGo/AlphaZero : A battu les champions mondiaux de Go et échiquier
Clé : S'est entraîné par auto-simulation sans données humaines
OpenAI Five : Bot Dota 2 qui a batti des champions du monde
AlphaStar : Maître niveau Grandmaster sur StarCraft II
4.2 Robotique
Manipulation d'objets : Robot qui apprend à saisir des objets inconnus
Locomotion : Robots quadrupèdes qui apprennent à marcher sur des terrains variés
Drones : Vol acrobatique autonome en s'entraînant en simulation
4.3 Sciences et Industrie
Découverte de médicaments : AlphaFold de DeepMind pour prédiction des protéines
Optimisation de datacenters : Réduction de 30% de la consommation énergétique (Google)
Trading algorithmique : Stratégies adaptatives (avec précautions)
Recommandation de contenu : Optimisation à long terme de l'engagement utilisateur
Contrôle de trafic : Optimisation des feux de signalisation pour fluidité
⚠️ Module 5 : Défis et Complexités
5.1 L'Exploration vs Exploitation
Le dilemme fondamental : Quand continuer à essayer de nouvelles choses vs. exploiter ce qui marche ?
Stratégies :
ε-greedy : Exploration aléatoire avec probabilité ε
Boltzmann exploration : Exploration probabiliste basée sur les valeurs
Curiosity-driven exploration : L'agent est récompensé pour découvrir de nouveaux états
5.2 Les Récompenses Éparses
Problème : Quand la récompense vient rarement (ex : gagner une partie d'échecs en 50 coups).
Solution : Apprentissage par imitation (apprendre d'expert humain d'abord)
Solution : Récompenses intermédiaires (shaping)
5.3 La Reproductibilité
Le RL est très sensible :
Aux hyperparamètres (taux d'apprentissage, etc.)
Aux initialisations aléatoires
À la chance de l'exploration
Conseil pratique : Toujours exécuter plusieurs essais avec différentes graines aléatoires.
5.4 La Simulation vers Réel (Sim-to-Real)
Problème : Un agent qui marche parfaitement en simulation échoue souvent dans le monde réel.
Solutions :
Domain Randomization : Varier les paramètres de simulation (masse, friction) pendant l'entraînement
Progressive augmentation de la complexité
Apprentissage few-shot : Quelques ajustements sur le robot réel
 Module 6 : Outils et Écosystème
6.1 Environnements de Simulation
OpenAI Gym : Standard historique (MuJoCo, Atari, Classic Control)
Gymnasium : Fork maintenu de Gym
PettingZoo : Pour RL multi-agents
DeepMind Control Suite : Tâches robotiques continues
Isaac Gym : Simulation GPU-accelérée (NVIDIA)
6.2 Librairies d'Implémentation
Stable-Baselines3 : Implémentations robustes et documentées (parfait pour débuter)
RLlib : Échelle massive, multi-agents (Ray)
CleanRL : Implémentations simples et lisibles (éducation)
Tianshou : Version PyTorch moderne
Sample Factory : Pour des vitesses d'entraînement extrêmes
6.3 Pour Commencer un Projet
Choisir un environnement simple : CartPole, LunarLander
Commencer avec un algorithme robuste : PPO
Utiliser une librairie éprouvée : Stable-Baselines3
Visualiser les courbes d'apprentissage : TensorBoard/Wandb
Itérer progressivement : Augmenter la complexité lentement
 Module 7 : Travaux Pratiques et Projet Étudiant
Projet Progressif Recommandé
Niveau Débutant :
Objectif : Faire tenir un bâton vertical (CartPole)
Algorithme : Q-learning simple ou DQN
Challenge : Régler l'exploration
Niveau Intermédiaire :
Objectif : Atteindre la cible sur la lune (LunarLander)
Algorithme : PPO ou A2C
Challenge : Gérer les actions continues
Niveau Avancé :
Objectif : Créer un agent multi-tâches
Algorithme : DQN amélioré (Dueling, Double, Prioritized Replay)
Challenge : Généralisation, réutilisation des connaissances
Projet Créatif :
Idée : Entraîner un agent à jouer à un jeu que vous aimez
Approche : Enregistrer vos propres parties (imitation), puis RL
Outils : Retro Gym pour les anciens jeux
 Module 8 : Perspectives et Recherches Actuelles
8.1 Apprentissage par Renforcement Multi-Agents
Agents qui apprennent simultanément dans un même environnement
Émergence de comportements complexes : Coopération, compétition, communication
Applications : Économie, trafic, systèmes distribués
8.2 Apprentissage par Renforcement Hors Ligne (Offline RL)
Apprendre uniquement à partir de données historiques sans nouvelle interaction
Crucial : Pour la médecine, l'automobile (où l'exploration est dangereuse)
Défi : Gérer la distribution des données
8.3 RL et Modèles du Monde (World Models)
L'agent apprend un modèle interne de son environnement
Planifie dans sa tête avant d'agir
Plus efficace : Moins d'interactions réelles nécessaires
Exemple : Dreamer, MuZero
8.4 RL avec Retour Humain (RLHF)
Utiliser des préférences humaines comme signal de récompense
Clé derrière ChatGPT : Affiner les LLMs avec feedback humain
Défi : Échelle et cohérence du feedback
8.5 Sécurité et Alignement
Problème : Comment s'assurer que l'agent ne "triche" pas sur la récompense ?
Exemple : Un robot qui doit nettoyer ne doit pas cacher la poussière
Recherche active : Reward shaping, apprentissage inverse, robustesse
Conseils Pédagogiques pour l'Enseignant
Pour Rendre le Cours Vivant
Démos interactives : Montrez des vidéos d'agents qui apprennent (TensorBoard live)
Concours étudiants : Battle de bots sur un jeu simple
Guest speakers : Chercheurs en RL ou ingénieurs industriels
Projets ouverts : Laissez les étudiants choisir leur environnement
Visualisations : Dessiner les réseaux, les états, les flux de décision
Pièges à Éviter
 Ne pas commencer par les équations complexes
Ne pas ignorer l'importance de l'ingénierie de récompense
 Ne pas sous-estimer la variance et la difficulté de reproduction
 Insister sur la pensée critique : Le RL n'est pas une solution miracle
 Résumé Final et Points Clés à Retenir
Le RL est :
Une boucle d'interaction Agent-Environnement
Guidé par des récompenses scalaires
Basé sur l'exploration intelligente
Puissant mais délicat à régler
En pleine expansion avec le Deep Learning
Pour réussir en RL :
Comprendre intimement votre environnement
Concevoir des récompenses intelligentes (reward engineering)
Expérimenter beaucoup (c'est itératif)
Visualiser tout (états, récompenses, politiques)
Être patient (l'apprentissage prend du temps)
 Ressources Complémentaires
Livres :
"Reinforcement Learning: An Introduction" (Sutton & Barto) - LA bible
"Deep Reinforcement Learning Hands-On" (Max Lapan)
Cours en ligne :
CS285 de Berkeley (Deep RL)
DeepMind x UCL RL Lectures (YouTube)
Communauté :
Reddit r/reinforcementlearning
Papers With Code (section RL)