APPRENTISSAGE PAR RENFORCEMENT (Reinforcement Learning)
üìö Module 1 : Fondements et Concepts Cl√©s
1.1 D√©finition et Philosophie
L'apprentissage par renforcement est une branche de l'intelligence artificielle o√π un agent apprend √† prendre des d√©cisions en interagissant avec un environnement. Contrairement √† l'apprentissage supervis√© o√π on fournit des exemples √©tiquet√©s, ici l'apprentissage se fait par essai et erreur, guid√© par des r√©compenses et des punitions.
Analogie p√©dagogique : Imaginez un enfant qui apprend √† marcher. Il fait un pas (action), observe s'il avance ou tombe (√©tat suivant), et re√ßoit un feedback (joie d'avancer vs. douleur de tomber). Progressivement, il optimise sa "politique" de marche.
1.2 Les Cinq Composants Essentiels
A. L'Agent (Agent)
L'entit√© qui apprend et prend des d√©cisions
Peut √™tre un robot, un logiciel, un joueur de jeux
Poss√®de une "politique" qui guide ses choix
B. L'Environnement (Environment)
Le monde avec lequel l'agent interagit
Peut √™tre simul√© (jeu vid√©o) ou r√©el (route pour une voiture autonome)
R√©pond aux actions de l'agent
C. Les √âtats (States)
Repr√©sentation de la situation actuelle
Exemple : position d'un joueur sur un √©chiquier, image per√ßue par une cam√©ra
L'espace des √©tats peut √™tre fini ou infini
D. Les Actions (Actions)
Choix possibles pour l'agent √† chaque √©tat
Exemple : avancer, reculer, tourner √† gauche
Peut √™tre discret (quelques boutons) ou continu (angle de volant)
E. Les R√©compenses (Rewards)
Signal scalaire imm√©diat re√ßu apr√®s chaque action
Crucial : C'est le seul signal d'apprentissage !
Une r√©compense positive renforce un comportement, une n√©gative le punit
1.3 Processus de D√©cision Markovien (MDP)
Le MDP est le cadre math√©matique qui formalise le probl√®me. M√™me sans formules, comprenez qu'il garantit que l'√©tat suivant ne d√©pend que de l'√©tat actuel et de l'action prise (propri√©t√© de Markov).
Intuition : Pour d√©cider quoi faire maintenant, vous n'avez pas besoin de tout l'historique, juste de la situation pr√©sente.
üéØ Module 2 : Les Trois Approches Principales
2.1 M√©thodes Bas√©es sur la Valeur (Value-Based)
Principe : Apprendre √† √©valuer chaque situation plut√¥t que d'agir directement.
Id√©e centrale : On calcule la "valeur" de chaque √©tat ou √©tat-action : "Si je suis ici et que je fais √ßa, √† long terme, combien de r√©compense puis-je esp√©rer ?"
Q-Learning (Apprentissage Q) :
Cr√©e une "table Q" qui stocke la qualit√© de chaque action dans chaque √©tat
D√©marre avec des valeurs al√©atoires, puis les affine par exploration
Phase d'exploration : Essayer de nouvelles actions (faire des erreurs)
Phase d'exploitation : Utiliser ce qui marche le mieux
Dilemme exploration/exploitation : Le coeur du challenge !
Analogie : Vous essayez diff√©rents restaurants dans une ville. Au d√©but, vous explorez al√©atoirement. Progressivement, vous apprenez quels restaurants sont bons (valeur √©lev√©e) et y revenez plus souvent.
2.2 M√©thodes Bas√©es sur la Politique (Policy-Based)
Principe : Apprendre directement la politique (la strat√©gie) sans √©valuer les √©tats interm√©diairement.
Policy Gradient :
Param√©trise la politique directement (ex : probabilit√© de chaque action)
Ajuste les param√®tres pour maximiser les r√©compenses attendues
Plus stable sur les grands espaces d'actions continus
Avantage : Plus naturel pour les actions continues (ex : tourner le volant de 17.3¬∞).
2.3 M√©thodes Acteur-Critique (Actor-Critic)
Principe : Combiner les deux approches pr√©c√©dentes.
Deux r√¥les :
L'Acteur : D√©cide quelle action prendre (policy-based)
Le Critique : √âvalue si la d√©cision √©tait bonne (value-based)
Processus :
L'acteur propose une action
Le critique √©value l'action ("C'√©tait mieux que pr√©vu !")
L'acteur ajuste sa strat√©gie selon le feedback du critique
Analogie : Un √©l√®ve (acteur) r√©pond √† une question, le professeur (critique) donne un feedback constructif, l'√©l√®ves' am√©liore.
üß† Module 3 : Apprentissage par Renforcement Profond (Deep RL)
3.1 Quand les R√©seaux de Neurones Entrent en Jeu
Le Deep RL combine le RL avec des r√©seaux de neurones pour r√©soudre des probl√®mes avec des espaces d'√©tats immenses (images, sons).
Probl√®me de la "mal√©diction de la dimensionnalit√©" :
Un jeu d'√©checs a plus d'√©tats qu'atomes dans l'univers observable
Impossible de stocker une table Q dans la m√©moire
Solution : Utiliser un r√©seau neuronal qui approxime la fonction de valeur ou la politique.
3.2 Algorithmes Cl√©s du Deep RL
DQN (Deep Q-Network) :
Utilise un CNN pour estimer les valeurs Q √† partir des pixels du jeu
Experience Replay : Stocke les exp√©riences pass√©es pour les r√©utiliser (√©chantillonnage efficace)
Target Network : Utilise deux r√©seaux pour stabiliser l'apprentissage
R√©volution : A battu des champions humains au jeu Atari juste en regardant les pixels
A3C (Asynchronous Advantage Actor-Critic) :
Utilise plusieurs agents en parall√®le qui apprennent simultan√©ment
Chaque agent a une copie de l'environnement et √©change p√©riodiquement ses connaissances
Avantage : Plus rapide, plus stable, utilise mieux les CPU
PPO (Proximal Policy Optimization) :
Algorithme de r√©f√©rence aujourd'hui pour de nombreuses t√¢ches
Tr√®s stable, facile √† r√©gler (hyperparam√®tres robustes)
Utilis√© par OpenAI pour des robots simul√©s et des t√¢ches de contr√¥le
D√©monstration visuelle conceptuelle : Imaginez un robot qui apprend √† marcher. Au lieu de programmer chaque muscle, on le laisse essayer des milliers de fois. Son "cerveau" neuronal re√ßoit les r√©compenses (distance parcourue) et ajuste les connexions pour am√©liorer la d√©marche.
üéÆ Module 4 : Applications Concr√®tes et Captivantes
4.1 Jeux et D√©tente
AlphaGo/AlphaZero : A battu les champions mondiaux de Go et √©chiquier
Cl√© : S'est entra√Æn√© par auto-simulation sans donn√©es humaines
OpenAI Five : Bot Dota 2 qui a batti des champions du monde
AlphaStar : Ma√Ætre niveau Grandmaster sur StarCraft II
4.2 Robotique
Manipulation d'objets : Robot qui apprend √† saisir des objets inconnus
Locomotion : Robots quadrup√®des qui apprennent √† marcher sur des terrains vari√©s
Drones : Vol acrobatique autonome en s'entra√Ænant en simulation
4.3 Sciences et Industrie
D√©couverte de m√©dicaments : AlphaFold de DeepMind pour pr√©diction des prot√©ines
Optimisation de datacenters : R√©duction de 30% de la consommation √©nerg√©tique (Google)
Trading algorithmique : Strat√©gies adaptatives (avec pr√©cautions)
Recommandation de contenu : Optimisation √† long terme de l'engagement utilisateur
Contr√¥le de trafic : Optimisation des feux de signalisation pour fluidit√©
‚ö†Ô∏è Module 5 : D√©fis et Complexit√©s
5.1 L'Exploration vs Exploitation
Le dilemme fondamental : Quand continuer √† essayer de nouvelles choses vs. exploiter ce qui marche ?
Strat√©gies :
Œµ-greedy : Exploration al√©atoire avec probabilit√© Œµ
Boltzmann exploration : Exploration probabiliste bas√©e sur les valeurs
Curiosity-driven exploration : L'agent est r√©compens√© pour d√©couvrir de nouveaux √©tats
5.2 Les R√©compenses √âparses
Probl√®me : Quand la r√©compense vient rarement (ex : gagner une partie d'√©checs en 50 coups).
Solution : Apprentissage par imitation (apprendre d'expert humain d'abord)
Solution : R√©compenses interm√©diaires (shaping)
5.3 La Reproductibilit√©
Le RL est tr√®s sensible :
Aux hyperparam√®tres (taux d'apprentissage, etc.)
Aux initialisations al√©atoires
√Ä la chance de l'exploration
Conseil pratique : Toujours ex√©cuter plusieurs essais avec diff√©rentes graines al√©atoires.
5.4 La Simulation vers R√©el (Sim-to-Real)
Probl√®me : Un agent qui marche parfaitement en simulation √©choue souvent dans le monde r√©el.
Solutions :
Domain Randomization : Varier les param√®tres de simulation (masse, friction) pendant l'entra√Ænement
Progressive augmentation de la complexit√©
Apprentissage few-shot : Quelques ajustements sur le robot r√©el
üõ†Ô∏è Module 6 : Outils et √âcosyst√®me
6.1 Environnements de Simulation
OpenAI Gym : Standard historique (MuJoCo, Atari, Classic Control)
Gymnasium : Fork maintenu de Gym
PettingZoo : Pour RL multi-agents
DeepMind Control Suite : T√¢ches robotiques continues
Isaac Gym : Simulation GPU-accel√©r√©e (NVIDIA)
6.2 Librairies d'Impl√©mentation
Stable-Baselines3 : Impl√©mentations robustes et document√©es (parfait pour d√©buter)
RLlib : √âchelle massive, multi-agents (Ray)
CleanRL : Impl√©mentations simples et lisibles (√©ducation)
Tianshou : Version PyTorch moderne
Sample Factory : Pour des vitesses d'entra√Ænement extr√™mes
6.3 Pour Commencer un Projet
Choisir un environnement simple : CartPole, LunarLander
Commencer avec un algorithme robuste : PPO
Utiliser une librairie √©prouv√©e : Stable-Baselines3
Visualiser les courbes d'apprentissage : TensorBoard/Wandb
It√©rer progressivement : Augmenter la complexit√© lentement
üéì Module 7 : Travaux Pratiques et Projet √âtudiant
Projet Progressif Recommand√©
Niveau D√©butant :
Objectif : Faire tenir un b√¢ton vertical (CartPole)
Algorithme : Q-learning simple ou DQN
Challenge : R√©gler l'exploration
Niveau Interm√©diaire :
Objectif : Atteindre la cible sur la lune (LunarLander)
Algorithme : PPO ou A2C
Challenge : G√©rer les actions continues
Niveau Avanc√© :
Objectif : Cr√©er un agent multi-t√¢ches
Algorithme : DQN am√©lior√© (Dueling, Double, Prioritized Replay)
Challenge : G√©n√©ralisation, r√©utilisation des connaissances
Projet Cr√©atif :
Id√©e : Entra√Æner un agent √† jouer √† un jeu que vous aimez
Approche : Enregistrer vos propres parties (imitation), puis RL
Outils : Retro Gym pour les anciens jeux
üîÆ Module 8 : Perspectives et Recherches Actuelles
8.1 Apprentissage par Renforcement Multi-Agents
Agents qui apprennent simultan√©ment dans un m√™me environnement
√âmergence de comportements complexes : Coop√©ration, comp√©tition, communication
Applications : √âconomie, trafic, syst√®mes distribu√©s
8.2 Apprentissage par Renforcement Hors Ligne (Offline RL)
Apprendre uniquement √† partir de donn√©es historiques sans nouvelle interaction
Crucial : Pour la m√©decine, l'automobile (o√π l'exploration est dangereuse)
D√©fi : G√©rer la distribution des donn√©es
8.3 RL et Mod√®les du Monde (World Models)
L'agent apprend un mod√®le interne de son environnement
Planifie dans sa t√™te avant d'agir
Plus efficace : Moins d'interactions r√©elles n√©cessaires
Exemple : Dreamer, MuZero
8.4 RL avec Retour Humain (RLHF)
Utiliser des pr√©f√©rences humaines comme signal de r√©compense
Cl√© derri√®re ChatGPT : Affiner les LLMs avec feedback humain
D√©fi : √âchelle et coh√©rence du feedback
8.5 S√©curit√© et Alignement
Probl√®me : Comment s'assurer que l'agent ne "triche" pas sur la r√©compense ?
Exemple : Un robot qui doit nettoyer ne doit pas cacher la poussi√®re
Recherche active : Reward shaping, apprentissage inverse, robustesse
üìñ Conseils P√©dagogiques pour l'Enseignant
Pour Rendre le Cours Vivant
D√©mos interactives : Montrez des vid√©os d'agents qui apprennent (TensorBoard live)
Concours √©tudiants : Battle de bots sur un jeu simple
Guest speakers : Chercheurs en RL ou ing√©nieurs industriels
Projets ouverts : Laissez les √©tudiants choisir leur environnement
Visualisations : Dessiner les r√©seaux, les √©tats, les flux de d√©cision
Pi√®ges √† √âviter
‚ùå Ne pas commencer par les √©quations complexes
‚ùå Ne pas ignorer l'importance de l'ing√©nierie de r√©compense
‚ùå Ne pas sous-estimer la variance et la difficult√© de reproduction
‚úÖ Insister sur la pens√©e critique : Le RL n'est pas une solution miracle
üìù R√©sum√© Final et Points Cl√©s √† Retenir
Le RL est :
Une boucle d'interaction Agent-Environnement
Guid√© par des r√©compenses scalaires
Bas√© sur l'exploration intelligente
Puissant mais d√©licat √† r√©gler
En pleine expansion avec le Deep Learning
Pour r√©ussir en RL :
Comprendre intimement votre environnement
Concevoir des r√©compenses intelligentes (reward engineering)
Exp√©rimenter beaucoup (c'est it√©ratif)
Visualiser tout (√©tats, r√©compenses, politiques)
√ätre patient (l'apprentissage prend du temps)
üìö Ressources Compl√©mentaires
Livres :
"Reinforcement Learning: An Introduction" (Sutton & Barto) - LA bible
"Deep Reinforcement Learning Hands-On" (Max Lapan)
Cours en ligne :
CS285 de Berkeley (Deep RL)
DeepMind x UCL RL Lectures (YouTube)
Communaut√© :
Reddit r/reinforcementlearning
Papers With Code (section RL)
